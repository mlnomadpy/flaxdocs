---
sidebar_position: 0
---

# Distributed Training Overview

Scale your Flax NNX models across multiple devices with JAX's powerful parallelism primitives. This guide covers everything from simple data parallelism to advanced sharding strategies for training models at any scale.

## Quick Decision Guide

**Choose your parallelism strategy:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Does your model fit on a single device? â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚
       YES            NO
        â”‚             â”‚
        â–¼             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Data  â”‚    â”‚ Need sequential  â”‚
    â”‚Parallelâ”‚   â”‚ architecture?    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                    â”‚             â”‚
                   YES            NO
                    â”‚             â”‚
                    â–¼             â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚Pipeline  â”‚    â”‚FSDP or   â”‚
            â”‚Parallel  â”‚    â”‚Tensor    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚Parallel  â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Parallelism Strategies

### 1. Data Parallelism

**Replicate model, split data**

- âœ… Simplest to implement
- âœ… Perfect scaling for throughput
- âœ… No model changes needed
- âŒ Model must fit on single device

```python
@jax.pmap
def train_step(state, batch):
    # Each device processes different data
    grads = compute_gradients(state, batch)
    grads = jax.lax.pmean(grads, 'devices')  # Sync
    return state.apply_gradients(grads)
```

**ðŸ“š [Learn Data Parallelism â†’](./data-parallelism.md)**

### 2. SPMD / Automatic Sharding

**Flexible sharding with compiler optimization**

- âœ… Very flexible (any sharding pattern)
- âœ… Modern JAX best practice
- âœ… Automatic optimization
- âš ï¸ Requires understanding sharding

```python
mesh = Mesh(devices, axis_names=('data', 'model'))
sharding = NamedSharding(mesh, P('data', 'model'))

@jax.jit
def train_step(state, batch):
    # Compiler handles communication automatically
    return update(state, batch)
```

**ðŸ“š [Learn SPMD Sharding â†’](./spmd-sharding.md)**

### 3. Pipeline Parallelism

**Split model into stages**

- âœ… Train very large models
- âœ… Works with sequential architectures
- âŒ Pipeline bubbles (70-90% efficiency)
- âŒ Complex implementation

```python
# Stage 1 on Device 0
# Stage 2 on Device 1
# Stage 3 on Device 2
# Stage 4 on Device 3

# Microbatches flow through pipeline
```

**ðŸ“š [Learn Pipeline Parallelism â†’](./pipeline-parallelism.md)**

### 4. FSDP (Fully Sharded Data Parallel)

**Shard everything to save memory**

- âœ… Massive memory savings (NÃ— reduction)
- âœ… Train NÃ— larger models
- âŒ More communication overhead
- âš ï¸ Needs fast interconnect

```python
# Shard parameters across all devices
mesh = Mesh(devices, axis_names=('fsdp',))
params = shard_fsdp(params, mesh)

# Automatic all-gather and reduce-scatter
```

**ðŸ“š [Learn FSDP â†’](./fsdp-fully-sharded.md)**

## Strategy Comparison

| Strategy | Memory/Device | Throughput | Communication | Best For |
|----------|---------------|------------|---------------|----------|
| **Data Parallel** | Full model (P) | Excellent | O(P) once/step | Standard training |
| **SPMD** | Configurable | Excellent | Optimized | Flexible needs |
| **Pipeline** | P/N | Good (70-90%) | O(activations) | Very large models |
| **FSDP** | P/N | Good | O(2P) per layer | Memory constrained |

**P** = Model size, **N** = Number of devices

## Real-World Examples

### Example 1: ResNet-50 Training (25M params)

**Model fits easily on single GPU**

```python
# âœ… Best: Data Parallelism
# - Simple implementation
# - Perfect scaling
# - No memory concerns

# Configuration:
# - 8Ã— A100 GPUs
# - Batch size: 32/device = 256 total
# - Training time: 100% efficient
```

### Example 2: GPT-2 Medium (355M params)

**Model fits but tight on memory**

```python
# âœ… Best: SPMD with data parallelism
# - Flexible for future growth
# - Modern approach
# - Can add model parallelism if needed

# Configuration:
# - 8Ã— A100-40GB
# - Pure data parallel: P('data', None)
# - Or light tensor parallel: P('data', 'model') with mesh (4, 2)
```

### Example 3: GPT-3 Scale (175B params)

**Model way too large for single device**

```python
# âœ… Best: Combination strategy
# - FSDP for memory: 1024 devices
# - Pipeline: 8 stages
# - Tensor parallel: 4-way per stage

# Configuration:
# - 1024Ã— A100-80GB
# - Mesh: (8, 4, 32) = (pipeline, tensor, fsdp)
# - Per device: ~600MB
```

### Example 4: Vision Transformer (1B params)

**Sequential architecture, moderately large**

```python
# âœ… Good options:
# Option A: FSDP (if 16+ GPUs)
# - Memory: 1GB per device (16 GPUs)
# - Clean implementation

# Option B: Pipeline (if 4-8 GPUs)
# - 4 stages, 8 microbatches
# - Efficiency: 73%

# Configuration:
# - 8Ã— A100-40GB
# - Choose based on interconnect speed
```

## Combining Strategies

Many large-scale training runs combine multiple strategies:

### FSDP + Data Parallelism

```python
# Shard model parameters (FSDP)
# Each shard replica uses data parallelism

mesh = Mesh(devices, axis_names=('fsdp', 'data'))
# Shape: (8, 16) = 128 devices total
# 8-way FSDP, 16-way data parallel per shard
```

### Pipeline + Tensor Parallelism

```python
# Split model into pipeline stages
# Each stage uses tensor parallelism

# Stage 1: Devices 0-7 (8-way tensor parallel)
# Stage 2: Devices 8-15 (8-way tensor parallel)
# Stage 3: Devices 16-23 (8-way tensor parallel)
# Stage 4: Devices 24-31 (8-way tensor parallel)
```

### 3D Parallelism (Pipeline + Tensor + Data)

```python
# The ultimate combination for massive models

mesh = Mesh(devices, axis_names=('pipeline', 'tensor', 'data'))
# Shape: (8, 8, 16) = 1024 devices
# - 8 pipeline stages
# - 8-way tensor parallel per stage
# - 16-way data parallel

# Used by: GPT-3, PaLM, LLaMA-2
```

## Getting Started

### Step 1: Profile Your Model

```python
import jax
import jax.numpy as jnp
from flax import nnx

# Initialize model
model = YourModel(...)

# Check size
graphdef, params = nnx.split(model)
total_params = sum(p.size for p in jax.tree.leaves(params))
model_size_gb = total_params * 4 / 1e9  # float32

print(f"Model: {total_params/1e9:.2f}B parameters ({model_size_gb:.2f} GB)")

# Profile one training step
@jax.jit
def profile_step(state, batch):
    # Your training step
    pass

# Run once to compile
state, metrics = profile_step(state, batch)

# Time it
import time
start = time.time()
for _ in range(10):
    state, metrics = profile_step(state, batch)
elapsed = (time.time() - start) / 10

print(f"Step time: {elapsed*1000:.1f}ms")
```

### Step 2: Choose Strategy

Use the decision guide above based on:
- Model size vs device memory
- Number of available devices
- Interconnect speed
- Architecture (sequential or not)

### Step 3: Implement

Follow the detailed guides for your chosen strategy:

- ðŸš€ **Starting out?** â†’ [Data Parallelism](./data-parallelism.md)
- ðŸŽ¯ **Want flexibility?** â†’ [SPMD Sharding](./spmd-sharding.md)
- ðŸ’¾ **Need memory?** â†’ [FSDP](./fsdp-fully-sharded.md)
- ðŸ—ï¸ **Very large model?** â†’ [Pipeline Parallelism](./pipeline-parallelism.md)

### Step 4: Optimize

After basic implementation works:

1. **Profile** with `jax.profiler.trace()`
2. **Check device utilization** (should be >80%)
3. **Adjust batch size** (larger = better efficiency)
4. **Enable mixed precision** (bfloat16)
5. **Tune communication** (see Best Practices)

## Common Pitfalls

### âŒ Wrong: Using pmap for large models

```python
# Model: 10B parameters, won't fit on single GPU!
@jax.pmap
def train_step(state, batch):
    # Each device needs full 10B model = OOM!
    pass
```

**âœ… Use FSDP or Pipeline Parallelism instead**

### âŒ Wrong: Too few microbatches with pipeline

```python
# 4 pipeline stages, only 2 microbatches
# Efficiency: 2/(2+4-1) = 40% (terrible!)
```

**âœ… Use 4Ã— stages microbatches minimum (16 for 4 stages)**

### âŒ Wrong: FSDP with slow interconnect

```python
# Using FSDP over 10Gb Ethernet
# Communication time > compute time!
```

**âœ… FSDP needs NVLink or InfiniBand (100+ GB/s)**

### âŒ Wrong: Not accounting for optimizer state

```python
# Model: 10GB
# Fits on A100-40GB?
# NO! Adam needs: 10GB params + 10GB grads + 20GB optimizer = 40GB
```

**âœ… Budget 4Ã— model size for training (params + grads + optimizer)**

## Scaling Laws

### Data Parallelism Scaling

```
Throughput = single_device_throughput Ã— num_devices Ã— efficiency

# Efficiency typically:
# - 2-4 devices: 95-98%
# - 8 devices: 90-95%
# - 16+ devices: 85-92%
# (Depends on model size and interconnect)
```

### FSDP Scaling

```
Max_model_size = device_memory Ã— num_devices / 4

# Examples (A100-40GB):
# - 8 devices: 80GB model (20B params)
# - 64 devices: 640GB model (160B params)
# - 1024 devices: 10TB model (2.5T params)
```

### Pipeline Efficiency

```
Efficiency = M / (M + S - 1)
where M = microbatches, S = stages

# To reach 90% efficiency:
M â‰¥ 9 Ã— S - 9

# Examples:
# 4 stages: Need 27+ microbatches for 90%
# 8 stages: Need 63+ microbatches for 90%
```

## Hardware Considerations

### Interconnect Speed

| Interconnect | Bandwidth | Good For |
|--------------|-----------|----------|
| **NVLink (V100)** | 300 GB/s | All strategies âœ… |
| **NVLink (A100)** | 600 GB/s | All strategies âœ…âœ… |
| **NVLink (H100)** | 900 GB/s | All strategies âœ…âœ…âœ… |
| **PCIe 4.0** | 64 GB/s | Data parallel only |
| **10Gb Ethernet** | 1.25 GB/s | Single device only |
| **InfiniBand** | 200 GB/s | All strategies âœ…âœ… |

### Memory Hierarchy

```
Device Memory (fast, small):
  â”œâ”€ L2 Cache: ~40-80MB (fastest)
  â”œâ”€ HBM: 40-80GB (fast)
  â””â”€ When full â†’ OOM!

Host Memory (slow, large):
  â””â”€ RAM: 100s of GB (for data loading)

Storage (slowest, largest):
  â””â”€ Disk: TBs (for dataset)
```

**Key insight:** Training happens in device memory. Must fit model + gradients + optimizer + activations.

## Monitoring Training

### Essential Metrics

```python
# 1. Loss / Accuracy (correctness)
# 2. Step time (efficiency)
# 3. Device utilization (>80% ideal)
# 4. Memory usage (should be high but not OOM)
# 5. Communication time (should be <30% of step time)

# Log these every N steps:
if step % 100 == 0:
    metrics = {
        'loss': float(loss),
        'accuracy': float(accuracy),
        'step_time_ms': step_time * 1000,
        'device_utilization': utilization,
    }
    print(metrics)
```

### Profiling

```python
# Profile to find bottlenecks
from jax import profiler

with profiler.trace("/tmp/jax-trace"):
    for _ in range(10):
        state = train_step(state, batch)

# View in TensorBoard:
# tensorboard --logdir=/tmp/jax-trace

# Look for:
# - Computation time (should be high)
# - Communication time (minimize)
# - Idle time (minimize)
```

## Next Steps

1. **Start simple:** [Data Parallelism Guide](./data-parallelism.md)
2. **Go modern:** [SPMD Sharding Guide](./spmd-sharding.md)
3. **Scale up:** [FSDP Guide](./fsdp-fully-sharded.md) or [Pipeline Guide](./pipeline-parallelism.md)
4. **Optimize:** [Best Practices](./best-practices.md) (coming soon)

## Example Code

Check out our complete, runnable examples:

- [`examples/16_data_parallel_pmap.py`](../../examples/16_data_parallel_pmap.py) - Data parallelism with pmap
- [`examples/17_sharding_spmd.py`](../../examples/17_sharding_spmd.py) - SPMD automatic sharding
- [`examples/18_pipeline_parallelism.py`](../../examples/18_pipeline_parallelism.py) - Pipeline parallelism
- [`examples/19_fsdp_sharding.py`](../../examples/19_fsdp_sharding.py) - FSDP fully sharded training

Each example is self-contained and includes detailed comments explaining what's happening under the hood.

## Further Reading

- [JAX Documentation on Parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)
- [Google Cloud TPU Guide](https://cloud.google.com/tpu/docs/jax-pods)
- [Megatron-LM Paper](https://arxiv.org/abs/1909.08053) (Pipeline + Tensor Parallelism)
- [ZeRO Paper](https://arxiv.org/abs/1910.02054) (FSDP inspiration)
- [GPipe Paper](https://arxiv.org/abs/1811.06965) (Pipeline Parallelism)
